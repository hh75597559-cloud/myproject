import os
import streamlit as st
import streamlit.components.v1 as components

from LLM import (
    is_similar,
    get_llm_backend,
    generate_with_openai,
    generate_with_gemini,
    gather_context,
    extract_questions,
    parse_mc_questions,
    parse_eval,
    get_chat_llm,
    get_llm_backend,
    get_chat_llm,
    hist_pairs,
    hist_text,
    summarize_docs
)


st.set_page_config(page_title="ì¦ì°©", layout="wide")

st.header("6) ì¦ì°© (CVD/PVD/ALD)")

CATEGORY_NAME = "ì¦ì°©"

st.subheader("ê°œìš”")
st.write(
    "ì›¨ì´í¼ í‘œë©´ì— ì›í•˜ëŠ” ì¬ë£Œì˜ ë°•ë§‰ì„ í˜•ì„±í•˜ëŠ” ê³µì •ì…ë‹ˆë‹¤. ë¬¼ë¦¬ì  ì¦ì°©(PVD: ìŠ¤í¼í„°/ì¦ë°œ), "
    "í™”í•™ì  ì¦ì°©(CVD: LPCVD/PECVD/RTCVD), ì›ìì¸µ ì¦ì°©(ALD)ì„ ì‚¬ìš©í•˜ë©°, "
    "ì¡°ì„±Â·ë‘ê»˜Â·ê· ì¼ë„Â·ì‘ë ¥Â·ì½˜í¬ë©€ë¦¬í‹°Â·ê²°í•¨/íŒŒí‹°í´ ê´€ë¦¬ê°€ í•µì‹¬ì…ë‹ˆë‹¤."
)

# í•µì‹¬ í¬ì¸íŠ¸ (íˆ´íŒ)
st.subheader("í•µì‹¬ í¬ì¸íŠ¸")
st.markdown("""
- <span title="ì˜¤ì—¼ ì œê±°Â·í‘œë©´ í™œì„±í™”Â·ì ‘ì°© í–¥ìƒÂ·í•µìƒì„± ì•ˆì •í™”">ì „ì²˜ë¦¬/ì‹œë“œ</span> â†’
  <span title="PVD/CVD/ALD ì¤‘ ëª©ì ì— ë§ëŠ” ë°©ì‹ ì„ íƒÂ·ë ˆì‹œí”¼ ì„¤ì •">ë§‰ í˜•ì„±</span> â†’
  <span title="ë‘ê»˜Â·ì¡°ì„±Â·ì‘ë ¥Â·ì…ì ìµœì†Œí™”ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ìµœì í™”">ì¡°ê±´ ìµœì í™”</span> â†’
  <span title="ì–´ë‹/í”Œë¼ì¦ˆë§ˆ ë“±ìœ¼ë¡œ ë§‰ì§ˆ ê°œì„ Â·ê²°í•©/ë°€ë„ í–¥ìƒ">í›„ì²˜ë¦¬</span> â†’
  <span title="ë‘ê»˜Â·ì¡°ì„±Â·ì‘ë ¥Â·ê²°í•¨/íŒŒí‹°í´ ì •ëŸ‰í™” ë° í”¼ë“œë°±">ê²€ì‚¬/ê³„ì¸¡</span>
""", unsafe_allow_html=True)
st.markdown("- í•µì‹¬ ì§€í‘œ: ë‘ê»˜/ê· ì¼ë„, ë§‰ì§ˆ(ë°€ë„/ì¡°ì„±/ê²°í•©), ì‘ë ¥(ì¸ì¥/ì••ì¶•), ì½˜í¬ë©€ë¦¬í‹°/ìŠ¤í… ì»¤ë²„ë¦¬ì§€, íŒŒí‹°í´/ë””í™íŠ¸, í‘œë©´ ê±°ì¹ ê¸°(Ra/RMS)")

# í”„ë¡œì„¸ìŠ¤(ê°€ë¡œ ìŠ¤í¬ë¡¤ ì¹´ë“œ)
st.subheader("í”„ë¡œì„¸ìŠ¤(ê°€ë¡œ ìŠ¤í¬ë¡¤ ì¹´ë“œ)")
steps = ["ì „ì²˜ë¦¬/ì‹œë“œ", "ë§‰ í˜•ì„±(PVD/CVD/ALD)", "ì¡°ê±´ ìµœì í™”", "í›„ì²˜ë¦¬", "ê²€ì‚¬/ê³„ì¸¡"]

html = """
<div style="overflow-x:auto; padding:6px 0;">
  <div style="display:flex; gap:12px; align-items:center; min-height:96px;">{items}</div>
</div>
"""
chip = """
<div style="flex:0 0 auto; min-width:200px; max-width:260px;
            padding:12px 14px; border:1.5px solid #d0d4dc; border-radius:14px;
            box-shadow:0 1px 3px rgba(0,0,0,0.06); background:#fff; text-align:center;">
  <div style="font-size:14px; font-weight:600;">{label}</div>
</div>
<div style="flex:0 0 auto; font-size:20px; margin:0 2px;">âœ</div>
"""
items = "".join(
    (chip.format(label=s) if i < len(steps)-1 else chip.replace(
        '<div style="flex:0 0 auto; font-size:20px; margin:0 2px;">âœ</div>', ''
    ).format(label=s))
    for i, s in enumerate(steps)
)
components.html(html.format(items=items), height=120, scrolling=False)

# ê³µì • ë‹¨ê³„ ì„¤ëª…
st.subheader("ê³µì • ë‹¨ê³„ ì„¤ëª… ë° ì§„ë„ ê´€ë¦¬")
# ë‹¨ê³„ë³„ ì •ë³´
steps_data = [
    {
        "name": "ì „ì²˜ë¦¬/ì‹œë“œ",
        "icon": "ğŸ§¼",
        "desc": """
ğŸ§¼ **ì „ì²˜ë¦¬/ì‹œë“œ(Pre-treatment / Seed)**

**ë¬´ì—‡ì„/ì™œ**
- ìœ ê¸°ë¬¼/ê¸ˆì† ì´ì˜¨/ìˆ˜ë¶„/íŒŒí‹°í´ ì œê±°, í‘œë©´ í™œì„±í™”ë¡œ **ì ‘ì°©ë ¥**ê³¼ **í•µìƒì„± ì•ˆì •ì„±** í™•ë³´.
- ì½˜í¬ë©€ ì¦ì°©ì´ ì–´ë ¤ìš´ êµ¬ì¡°ì—ì„œëŠ” **ì‹œë“œì¸µ**ìœ¼ë¡œ ì´ˆê¸° ì—°ì†ì„± ì œê³µ(íŠ¹íˆ ê¸ˆì† ë„ê¸ˆ/ECP ì „ ë‹¨ê³„).

**ì–´ë–»ê²Œ**
- ìŠµì‹: SC-1/SC-2, DHF(HF-last)ë¡œ ë„¤ì´í‹°ë¸Œ ì‚°í™”ë§‰ ì œì–´ â†’ DIW ë¦°ìŠ¤Â·ê±´ì¡°.
- ë“œë¼ì´: Ar í”Œë¼ì¦ˆë§ˆ í´ë¦°, Hâ‚‚/Nâ‚‚ í”Œë¼ì¦ˆë§ˆ ë¦¬ë•ì…˜, UV/ì˜¤ì¡´ ìœ ê¸° ì œê±°.
- ì ‘ì°© í–¥ìƒ: HMDS/SAM, í”„ë¼ì´ë¨¸, í”Œë¼ì¦ˆë§ˆ í™œì„±í™”.
- ì‹œë“œ: PVD/ALDë¡œ ìˆ˜~ìˆ˜ì‹­ nm ìˆ˜ì¤€ ê· ì¼/ì—°ì† ë§‰ í˜•ì„±.

**ì£¼ì˜ì‚¬í•­**
- DHF ê³¼ê³µì • â†’ í‘œë©´ ê±°ì¹ ê¸°â†‘/ë¯¸ì„¸ íŒ¨í„´ ì†ìƒ. HF-last í›„ **ì§€ì—° ìµœì†Œí™”**.
- í”Œë¼ì¦ˆë§ˆ/UV ê³¼ë…¸ì¶œ â†’ í‘œë©´ ë°ë¯¸ì§€/ê³¼ë„í•œ ì¹œìˆ˜í™”ë¡œ ì¬ì˜¤ì—¼ ë¦¬ìŠ¤í¬.
- ì‹œë“œ ë¶ˆì—°ì†/ì„¬í˜• ì„±ì¥ â†’ ì´í›„ ë§‰ì˜ ê³µê·¹/ì‹œì„ ì›ì¸.

ğŸ¯ **í•µì‹¬ ìš”ì•½**
- **ì²­ì •+í™œì„±í™”+ì ì ˆí•œ ì‹œë“œ**ê°€ ì¦ì°© ë§‰ì§ˆÂ·ì ‘ì°©ì˜ ì¶œë°œì .
"""
    },
    {
        "name": "ë§‰ í˜•ì„± (PVD/CVD/ALD)",
        "icon": "ğŸ› ï¸",
        "desc": """
ğŸ› ï¸ **ë§‰ í˜•ì„±(Deposition) â€” PVD/CVD/ALD ì„ íƒ**

**PVD (Physical Vapor Deposition)**
- ìŠ¤í¼í„°/ì¦ë°œ ê¸°ë°˜ **ë¼ì¸-ì˜¤ë¸Œ-ì‚¬ì´íŠ¸** ìš°ì„¸, ë¹ ë¥¸ ì¦ì°©, í•©ê¸ˆ/ê¸ˆì† ë°•ë§‰ì— ìœ ë¦¬.
- ë§¤ê°œë³€ìˆ˜: íƒ€ê¹ƒ ì „ë ¥, ì••ë ¥(Ar), ë°”ì´ì–´ìŠ¤, ê¸°íŒ ì˜¨ë„, ê±°ë¦¬, ë§ˆê·¸ë„¤íŠ¸ë¡ /íˆí„°.
- ì¥ì : ë†’ì€ ìˆœë„/ìƒì‚°ì„±, ì‰¬ìš´ ì¡°ì„± ì œì–´. ë‹¨ì : ì½˜í¬ë©€ë¦¬í‹°/í•˜ì´ AR ì¶©ì „ ì·¨ì•½.

**CVD (Chemical Vapor Deposition)**
- ê¸°ìƒ ì „êµ¬ì²´ ë°˜ì‘ìœ¼ë¡œ í•„ë¦„ ì„±ì¥(LPCVD/PECVD/RTCVD).
- ë§¤ê°œë³€ìˆ˜: ì˜¨ë„, ì••ë ¥, ì „êµ¬ì²´/ìºë¦¬ì–´ ìœ ëŸ‰, RF íŒŒì›Œ(PECVD), ë°˜ì‘/íê¸° ë¼ì¸ ì˜¨ë„.
- ì¥ì : ìŠ¤í… ì»¤ë²„ë¦¬ì§€â†‘, ëŒ€ë©´ì  ê· ì¼ë„â†‘. ë‹¨ì : ë¶ˆìˆœë¬¼/í•˜ì´ë“œë¡œê²/ì €ë°€ë„ ìœ„í—˜(íŠ¹íˆ ì €ì˜¨ PECVD).

**ALD (Atomic Layer Deposition)**
- **ìê¸° ì œí•œ ë°˜ì‘** ê¸°ë°˜ **ì‚¬ì´í´ì‹** ì¦ì°©: A í¼ì§€ B í¼ì§€ â€¦ (í‘œë©´ í¬í™”).
- ë§¤ê°œë³€ìˆ˜: ì‚¬ì´í´ ìˆ˜(=ë‘ê»˜), í„ìŠ¤/í¼ì§€ ì‹œê°„, ê¸°íŒ ì˜¨ë„(ìœˆë„ìš°), ì „êµ¬ì²´ ë°˜ì‘ì„±.
- ì¥ì : **íƒì›”í•œ ì½˜í¬ë©€ë¦¬í‹°**Â·ì›ìì¸µ ì œì–´Â·í•€í™€ ìµœì†Œ. ë‹¨ì : ì†ë„â†“, ì „êµ¬ì²´ ë¹„ìš©/ë…ì„± ê´€ë¦¬.

**ì£¼ì˜ì‚¬í•­**
- ì „êµ¬ì²´ ìˆœë„/ë¼ì¸ ì»¨ë””ì…˜ ë¶ˆëŸ‰ â†’ íŒŒí‹°í´/í´ë¦¬ë¨¸ ì”ë¥˜/í•µí˜•ì„± ì§€ì—°.
- PVDì˜ ì¬ë¹„ì‚°/ì„€ë„ì‰, CVDì˜ ê¸°ìƒ í´ë¦¬ë¨¸, ALDì˜ ë°˜ì‘ ë¯¸í¬í™”(ì–¸ë”í¼ì§€) ì£¼ì˜.
- í•˜ë¶€ë§‰/ì˜¨ë„ ìœˆë„ìš°/í”Œë¼ì¦ˆë§ˆ ë°ë¯¸ì§€/ì°¨ì§• ë“± ë””ë°”ì´ìŠ¤ ì œì•½ ê³ ë ¤.

ğŸ¯ **í•µì‹¬ ìš”ì•½**
- **PVD=ê¸ˆì† ë¹ ë¦„**, **CVD=ì»¤ë²„ë¦¬ì§€/ìƒì‚°ì„±**, **ALD=ì½˜í¬ë©€Â·ì •ë°€** â€” ëª©ì ì— ë§ì¶° ì„ íƒ.
"""
    },
    {
        "name": "ì¡°ê±´ ìµœì í™”",
        "icon": "âš™ï¸",
        "desc": """
âš™ï¸ **ì¡°ê±´ ìµœì í™”(Process Tuning)**

**ëª©í‘œ**
- ë‘ê»˜Â·ì¡°ì„±Â·ì‘ë ¥(ì¸ì¥/ì••ì¶•)Â·ë°€ë„Â·í‘œë©´ ê±°ì¹ ê¸°Â·ê²°í•¨/íŒŒí‹°í´ì„ **ë™ì‹œì—** ë§Œì¡±.

**íŒŒë¼ë¯¸í„° ì˜ˆì‹œ**
- ì˜¨ë„/ì••ë ¥/ìœ ëŸ‰/ì „ë ¥(RF/DC)/í”Œë¼ì¦ˆë§ˆ ë“€í‹°/ë°”ì´ì–´ìŠ¤.
- PVD: íŒŒì›ŒÂ·ì••ë ¥Â·ê¸°íŒ ë°”ì´ì–´ìŠ¤Â·ê¸°íŒ íšŒì „Â·ê°ë„ ì¦ì°©(ë¦¬ë‹ˆì–´ë¼ì´ì¦ˆ).
- CVD/ALD: ì „êµ¬ì²´/ì½”ë¦¬ì•¡í„´íŠ¸ í„ìŠ¤Â·í¼ì§€Â·ì‚¬ì´í´, ë¼ì¸ ì˜¨ë„, ì±”ë²„ ì»¨ë””ì…”ë‹.

**ì‘ë ¥/ë§‰ì§ˆ ì œì–´**
- ì˜¨ë„/ì „ë ¥/ì••ë ¥ ì¡°í•©ìœ¼ë¡œ ì‘ë ¥ ì œì–´(ë³´ìš°/ì»¤ë¸Œ ì¸¡ì • í”¼ë“œë°±).
- ALD ì‚¬ì´í´ íŒŒë¼ë¯¸í„°ë¡œ ë°€ë„/ê²°í•© ìƒíƒœ íŠœë‹(ì˜ˆ: ì‚°í™”ë¬¼ vs ì§ˆí™”ë¬¼).
- í•˜ë¶€ë§‰ê³¼ì˜ ê³„ë©´ ë°˜ì‘/ê°ˆë°”ë‹‰/ë””í“¨ì „ ê³ ë ¤(ë°°ë¦¬ì–´ í•„ìš” ì‹œ ë³„ë„ ë‹¨ê³„ ì¶”ê°€).

**ì£¼ì˜ì‚¬í•­**
- ê³¼ë„í•œ í”Œë¼ì¦ˆë§ˆ íŒŒì›Œ â†’ í‘œë©´ ì†ìƒ/ì°¨ì§•/íŠ¸ë© ì¦ê°€.
- í¼ì§€ ë¶€ì¡± â†’ ì „êµ¬ì²´ í¬ë¡œìŠ¤ë¦¬ì•¡ì…˜ìœ¼ë¡œ íŒŒí‹°í´/í´ë¦¬ë¨¸ ë°œìƒ.
- ë ˆì‹œí”¼ ë³€ê²½ ì‹œ **ì±”ë²„ ë©”ëª¨ë¦¬**/ë”ë¯¸ ì›¨ì´í¼ ì»¨ë””ì…”ë‹ í•„ìˆ˜.

ğŸ¯ **í•µì‹¬ ìš”ì•½**
- **ì‘ë ¥â€“ë°€ë„â€“ì»¤ë²„ë¦¬ì§€** íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì§€í‘œ(ë‘ê»˜/ì‘ë ¥/ê²°í•¨)ë¡œ ë‹«ëŠ” **ì‹¤í—˜ê³„íš(DoE)**ê°€ ê´€ê±´.
"""
    },
    {
        "name": "í›„ì²˜ë¦¬",
        "icon": "ğŸ”¥",
        "desc": """
ğŸ”¥ **í›„ì²˜ë¦¬(Post-treatment)**

**ëª©ì **
- ê²°í•©/ë°€ë„ í–¥ìƒ, ì”ë¥˜ ë¼ë””ì¹¼/ë¦¬ê°„ë“œ ì œê±°, ì „ê¸°Â·ê¸°ê³„ íŠ¹ì„± ê°œì„ , ì ‘ì´‰ì €í•­ ì €ê°.

**ë°©ë²•**
- ì–´ë‹: Nâ‚‚/Ar/Hâ‚‚(í¬ë°ê°€ìŠ¤) RTA/í¼ë‹ˆìŠ¤ â€” ìˆ˜ì†Œ íŒ¨ì‹œë² ì´ì…˜, ë°€ë„â†‘.
- í”Œë¼ì¦ˆë§ˆ: Hâ‚‚/Nâ‚‚/Oâ‚‚/Ar í”Œë¼ì¦ˆë§ˆë¡œ í‘œë©´ ê²°í•©/ì˜¤ì—¼ ì œê±°, í‘œë©´ ì—ë„ˆì§€ ì¡°ì ˆ.
- UV/ì˜¤ì¡´: ìœ ê¸° ì”ë¥˜ ì œê±°, ì¹œìˆ˜ì„±/ì†Œìˆ˜ì„± ì¡°ì ˆ.

**ì£¼ì˜ì‚¬í•­**
- ë©”íƒˆ/ìœ ì „ë§‰ì˜ **ì—´ ì•ˆì •ì„± ìœˆë„ìš°** ì¤€ìˆ˜(ìƒë³€í™”/ë””í“¨ì „/ë°•ë¦¬).
- ê³¼ë„í•œ Hâ‚‚ í”Œë¼ì¦ˆë§ˆ/ê³ ì˜¨ ì–´ë‹ â†’ ë°•ë§‰/ê³„ë©´ ì—´í™”Â·ê· ì—´Â·ìŠ¤íŠ¸ë ˆìŠ¤ ìƒìŠ¹.
- í›„ì²˜ë¦¬ë¡œ ì‘ë ¥ ë³€ë™ ê°€ëŠ¥ â†’ CMP/í¬í†  ì •ë ¬ì— ì˜í–¥.

ğŸ¯ **í•µì‹¬ ìš”ì•½**
- **ì–´ë‹/í”Œë¼ì¦ˆë§ˆ**ë¡œ ë§‰ì§ˆì„ ëŒì–´ì˜¬ë¦¬ë˜, **ì—´Â·í”Œë¼ì¦ˆë§ˆ ìœˆë„ìš°** ë‚´ì—ì„œ ìš´ì „.
"""
    },
    {
        "name": "ê²€ì‚¬/ê³„ì¸¡",
        "icon": "ğŸ”",
        "desc": """
ğŸ” **ê²€ì‚¬/ê³„ì¸¡(Metrology)**

**ë‘ê»˜/í˜•ìƒ**
- íƒ€ì›ê³„/ìŠ¤í™íŠ¸ëŸ¼ ë°˜ì‚¬ìœ¨/í”„ë¡œíŒŒì¼ëŸ¬, XRRë¡œ ë‘ê»˜Â·ë°€ë„Â·ì¡°ë„.
- ì½˜í¬ë©€ë¦¬í‹°/ìŠ¤í… ì»¤ë²„ë¦¬ì§€: í¬ë¡œìŠ¤ì„¹ì…˜ SEM/TEM, FIB ë‹¨ë©´.

**ì¡°ì„±/ê²°í•©**
- XPS/ToF-SIMS/FTIRë¡œ ì¡°ì„±/ê²°í•© ìƒíƒœ, ë¶ˆìˆœë¬¼( C/H/N/O ) ì •ëŸ‰.
- XRDë¡œ ê²°ì •ìƒ/ì‘ë ¥, ë¼ë§Œìœ¼ë¡œ ì‘ë ¥/ê²°í•© í™•ì¸.

**ê²°í•¨/íŒŒí‹°í´/ì‘ë ¥**
- íŒŒí‹°í´ ìŠ¤ìºë„ˆ/ê²€ì‚¬, í‘œë©´ ê±°ì¹ ê¸°(AFM), ë³´ìš°/ì»¤ë¸Œë¡œ ì‘ë ¥ ì¸¡ì •.
- ì „ê¸°ì : ëˆ„ì„¤/ìœ ì „ ì†ì‹¤/ì €í•­, ì½˜íƒ ì €í•­ Kelvin êµ¬ì¡°.

**ì£¼ì˜ì‚¬í•­**
- ë ˆí¼ëŸ°ìŠ¤ ì›¨ì´í¼/ì¸¡ì • ë ˆì‹œí”¼ ê³ ì •ìœ¼ë¡œ ë¡œíŠ¸ ê°„ ë¹„êµì„± í™•ë³´.
- ë°•ë§‰/ê³  AR êµ¬ì¡° ì¸¡ì •ì€ **ëª¨ë¸ ì˜ì¡´ì„±** í¬ë¯€ë¡œ ìƒí˜¸ ë³´ì™„ ì¸¡ì • ê¶Œì¥.

ğŸ¯ **í•µì‹¬ ìš”ì•½**
- **ë‘ê»˜Â·ì¡°ì„±Â·ì‘ë ¥Â·ê²°í•¨**ì„ ë‹¤ê°ë„ë¡œ ê³„ì¸¡í•˜ê³ , ê²°ê³¼ë¥¼ ì¦‰ì‹œ ë ˆì‹œí”¼ì— **í”¼ë“œë°±**.
"""
    },
]

# í˜ì´ì§€ ì§„ë„ ë²„í‚·
PAGE_PROGRESS_KEY = f"{CATEGORY_NAME}_progress"
st.session_state.pop("progress", None)
if PAGE_PROGRESS_KEY not in st.session_state:
    st.session_state[PAGE_PROGRESS_KEY] = {s["name"]: False for s in steps_data}
else:
    for s in steps_data:
        st.session_state[PAGE_PROGRESS_KEY].setdefault(s["name"], False)

# ë‹¨ê³„ë³„ ì„¤ëª… ë° ì²´í¬ë°•ìŠ¤
completed = 0
for s in steps_data:
    with st.expander(f"{s['icon']} {s['name']}"):
        st.write(s["desc"])
        checked = st.checkbox(
            "ì´ ë‹¨ê³„ í•™ìŠµ ì™„ë£Œ",
            value=st.session_state[PAGE_PROGRESS_KEY].get(s["name"], False),
            key=f"{CATEGORY_NAME}_{s['name']}" # â† ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ í‚¤ ì¶©ëŒ ë°©ì§€
        )
        st.session_state[PAGE_PROGRESS_KEY][s["name"]] = checked
        if checked:
            completed += 1

# ì „ì²´ ì§„ë„ìœ¨ í‘œì‹œ
total = len(steps_data)
percent = int((completed / total) * 100)
st.progress(percent)
st.caption(f"ğŸ“˜ í•™ìŠµ ì§„ë„: {completed} / {total} ë‹¨ê³„ ì™„ë£Œ ({percent}%)")

# ì§ˆì˜ì‘ë‹µ
st.subheader("ì§ˆì˜ì‘ë‹µ")

# ì§ˆì˜ì‘ë‹µ ìƒë‹¨ íˆ´ë°”: ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
c1, c2 = st.columns([1, 9])
with c1:
    if st.button("ëŒ€í™” ì´ˆê¸°í™”", key="btn_clear_qa", use_container_width=True, help="ì§ˆì˜ì‘ë‹µ ëŒ€í™” ë‚´ìš© ì „ì²´ ì‚­ì œ"):
        # ëŒ€í™” ì´ë ¥ ë¹„ìš°ê¸°
        st.session_state["chat_history"] = []

        st.toast("ì§ˆì˜ì‘ë‹µ ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ§¹")
        (st.rerun if hasattr(st, "rerun") else st.experimental_rerun)()


# PDF ìë£Œ ë„£ê¸°
if "vectorstore" not in st.session_state:
    st.info("ì„ë² ë”© ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤. ë©”ì¸ì—ì„œ PDF ì—…ë¡œë“œ â†’ ì„ë² ë”© ìƒì„± í›„ ì´ìš©í•˜ì„¸ìš”.")
else:
    if "qa_chain" not in st.session_state:
        # â¬‡ï¸ LLM.pyì˜ í•¨ìˆ˜ë¡œ ë°±ì—”ë“œ/ëª¨ë¸/LLMì„ ê°€ì ¸ì˜´.
        try:
            backend, model = get_llm_backend()   # "openai" | "gemini", ëª¨ë¸ ë¬¸ìì—´
        except Exception:
            backend = st.session_state.get("llm_backend", "openai")
            model   = st.session_state.get("llm_model", "gpt-4o-mini")

        try:
            llm = get_chat_llm(backend=backend, model=model, temperature=0.2)
        except Exception as e:
            st.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            llm = None

        retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 4})

        # PDFìš°ì„  PDFì— ë‚´ìš© ì—†ìœ¼ë©´ LLMì´ ì•Œì•„ì„œ ë‹µë³€í•´ì¤Œ
        st.session_state.retriever = retriever
        st.session_state.llm = llm
        st.session_state.qa_mode = "manual"   # ê¸°ë³¸ ìˆ˜ë™, CRC ë˜ë©´ "crc"ë¡œ ë³€ê²½

        if llm is not None:
            try:
                # â–¼ CRC ì‹œë„ (ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì§ì ‘ ë„˜ê¸¸ ìˆ˜ ìˆìŒ)
                from langchain.chains import ConversationalRetrievalChain
                from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

                prompt = ChatPromptTemplate.from_messages([
                    ("system",
                     "ë‹¹ì‹ ì˜ 1ì°¨ ì •ë³´ì›ì€ ì—…ë¡œë“œëœ PDFì…ë‹ˆë‹¤. "
                     "ê°€ëŠ¥í•˜ë©´ PDF ê·¼ê±°ë¥¼ ìš°ì„ í•˜ì—¬ ë‹µí•˜ê³ , ë¶€ì¡±í•˜ë©´ ì¼ë°˜ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ë˜ ê·¸ ì‚¬ì‹¤ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ í‘œì‹œí•˜ì‹­ì‹œì˜¤. "
                     "í•­ìƒ ì •ì¤‘í•œ í•œêµ­ì–´(ì¡´ëŒ“ë§)ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤."),
                    MessagesPlaceholder(variable_name="chat_history"),
                    ("human", "{question}")
                ])

                st.session_state.qa_chain = ConversationalRetrievalChain.from_llm(
                    llm=llm,
                    retriever=retriever,
                    return_source_documents=True,
                    combine_docs_chain_kwargs={"prompt": prompt},
                )
                st.session_state.qa_mode = "crc"
            except Exception:
                # ì •ë³´ ì—†ìœ¼ë©´ LLM ì•Œì•„ì„œ ë‹µë³€
                st.session_state.qa_chain = None

    # ì±„íŒ… ë‚´ì—­ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []   # [{'role':'user'|'assistant', 'content': str, 'sources': list}]

    # ê³¼ê±° ì±„íŒ… í‘œì‹œ (ìµœê·¼ 2ê°œë§Œ ë³´ì´ê³  ë‚˜ë¨¸ì§€ëŠ” í¼ì³ì„œ ë³¼ ìˆ˜ ìˆìŒ)
    if st.session_state.chat_history:
        older = st.session_state.chat_history[:-2]
        recent = st.session_state.chat_history[-2:]

        if older:
            with st.expander(f"ì´ì „ ëŒ€í™” ë³´ê¸° ({len(older)}ê°œ)", expanded=False):
                for msg in older:
                    with st.chat_message("user" if msg["role"]=="user" else "assistant"):
                        st.markdown(msg["content"])
                        if msg.get("sources"):
                            with st.popover("ì¶œì²˜ ë³´ê¸°"):
                                for i, meta in enumerate(msg["sources"], 1):
                                    st.caption(f"{i}. {meta}")

        for msg in recent:
            with st.chat_message("user" if msg["role"]=="user" else "assistant"):
                st.markdown(msg["content"])
                if msg.get("sources"):
                    with st.popover("ì¶œì²˜ ë³´ê¸°"):
                        for i, meta in enumerate(msg["sources"], 1):
                            st.caption(f"{i}. {meta}")

    # ì…ë ¥ì°½
    with st.form("qa_form", clear_on_submit=True):
        user_q = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦ (ì˜ˆ: EUVì™€ DUV ì°¨ì´)", key="qa_text")
        submitted = st.form_submit_button("Send")

    # âœ… ë²„íŠ¼ì„ ëˆŒë €ê³  ë¹„ì–´ ìˆì§€ ì•Šì„ ë•Œë§Œ ìƒì„±
    if submitted and user_q and user_q.strip():
        # 1) ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡ & í‘œì‹œ
        st.session_state.chat_history.append({"role": "user", "content": user_q})
        with st.chat_message("user"):
            st.markdown(user_q)
        # 2) ì‘ë‹µ ìƒì„±
        if st.session_state.get("qa_mode") == "crc" and st.session_state.get("qa_chain") is not None:
            # CRC ê²½ë¡œ: ëŒ€í™” ë§¥ë½ì„ chat_history ì¸ìë¡œ ì§ì ‘ ì „ë‹¬
            with st.chat_message("assistant"):
                with st.status("ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì¤‘...", expanded=False):
                    out = st.session_state.qa_chain({
                        "question": user_q,
                        "chat_history": hist_pairs(st.session_state.chat_history, limit_pairs=6)  # â† LLM.py í•¨ìˆ˜
                    })

                answer = out.get("answer") or out.get("result") or "ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"
                st.markdown(answer)

                # ì¶œì²˜ ìš”ì•½
                srcs = []
                for sdoc in (out.get("source_documents") or []):
                    meta = sdoc.metadata or {}
                    srcs.append(f"{meta.get('source','íŒŒì¼')} p.{meta.get('page','?')}")
                if srcs:
                    with st.popover("ì¶œì²˜ ë³´ê¸°"):
                        for i, meta in enumerate(srcs, 1):
                            st.caption(f"{i}. {meta}")
                # íˆìŠ¤í† ë¦¬ ì €ì¥
                st.session_state.chat_history.append({"role":"assistant", "content":answer, "sources":srcs})

        else:
            # ìˆ˜ë™ RAG í´ë°±: ë¬¸ì„œ ê²€ìƒ‰ + ëŒ€í™”ë§¥ë½ì„ ì§ì ‘ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
            with st.chat_message("assistant"):
                with st.status("ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì¤‘...", expanded=False):
                    llm = st.session_state.get("llm", None)
                    retriever = st.session_state.get("retriever", None)

                    docs = []
                    if retriever is not None:
                        try:
                            docs = retriever.get_relevant_documents(user_q)
                        except Exception:
                            docs = []

                    hist_txt = hist_text(st.session_state.chat_history, limit_pairs=6)  # â† LLM.py í•¨ìˆ˜
                    doc_block = summarize_docs(docs, max_chars=2400)                  # â† LLM.py í•¨ìˆ˜

                    if doc_block:
                        prompt = (
                            "ê·œì¹™:\n"
                            "1) ì•„ë˜ [PDF ë°œì·Œ]ì—ì„œ ë¨¼ì € ê·¼ê±°ë¥¼ ì°¾ê³  ë‹µí•˜ì‹­ì‹œì˜¤.\n"
                            "2) ì¶©ë¶„í•œ ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ê³ , ê·¸ ì‚¬ì‹¤ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ í‘œì‹œí•˜ì‹­ì‹œì˜¤.\n"
                            "3) í•œêµ­ì–´(ì¡´ëŒ“ë§)ë¡œ ê°„ê²°í•˜ê³  ì •í™•íˆ ë‹µí•˜ì‹­ì‹œì˜¤.\n\n"
                            f"[ëŒ€í™” ë§¥ë½]\n{hist_txt or '(ì´ì „ ëŒ€í™” ì—†ìŒ)'}\n\n"
                            f"[PDF ë°œì·Œ]\n{doc_block}\n\n"
                            f"[ì§ˆë¬¸]\n{user_q}\n"
                        )
                    else:
                        prompt = (
                            "ë‹¤ìŒì€ ìµœê·¼ ëŒ€í™”ì…ë‹ˆë‹¤.\n"
                            f"{hist_txt or '(ì´ì „ ëŒ€í™” ì—†ìŒ)'}\n\n"
                            "ì—…ë¡œë“œëœ PDFì—ì„œ ì¶©ë¶„í•œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì§€ì‹ìœ¼ë¡œ ë‹µí•˜ë˜, "
                            "ëª¨í˜¸í•˜ë©´ 'ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤'ë¼ê³  ë°í˜€ì£¼ì‹­ì‹œì˜¤. í•œêµ­ì–´(ì¡´ëŒ“ë§)ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤.\n"
                            f"[ì§ˆë¬¸] {user_q}"
                        )

                    if llm is None:
                        answer = "ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    else:
                        try:
                            out = llm.invoke(prompt)
                            answer = getattr(out, "content", None) or getattr(out, "text", None) or "ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"
                        except Exception:
                            answer = "ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                st.markdown(answer)

                # ì¶œì²˜ ìš”ì•½
                srcs = []
                for sdoc in (docs or []):
                    meta = getattr(sdoc, "metadata", {}) or {}
                    srcs.append(f"{meta.get('source','íŒŒì¼')} p.{meta.get('page','?')}")
                if srcs:
                    with st.popover("ì¶œì²˜ ë³´ê¸°"):
                        for i, meta in enumerate(srcs, 1):
                            st.caption(f"{i}. {meta}")

                # íˆìŠ¤í† ë¦¬ ì €ì¥
                st.session_state.chat_history.append({"role":"assistant", "content":answer, "sources":srcs})


# ëœë¤ ë¬¸ì œ ìƒì„±ê¸°  ì±„ì 
st.subheader("ëœë¤ ë¬¸ì œ ìƒì„±ê¸°")
CATEGORY_NAME = "í¬í† ë¦¬ì†Œê·¸ë˜í”¼"  # â† í˜ì´ì§€ ì£¼ì œëª…

# (ì¤‘ë³µ íšŒí”¼ìš©)
hist_key = f"{CATEGORY_NAME}_quiz_history"
if hist_key not in st.session_state:
    st.session_state[hist_key] = []  # ë¬¸ìì—´(ì„œìˆ í˜• ì§ˆë¬¸) ë˜ëŠ” MC ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì €ì¥

# ì„¤ì •
cols = st.columns(3)
difficulty   = cols[0].selectbox(
    "ë‚œì´ë„",
    ["ì´ˆê¸‰", "ê³ ê¸‰"],
    index=0,
    key=f"{CATEGORY_NAME}_difficulty"
)
n_items      = cols[1].selectbox(
    "ë¬¸í•­ ìˆ˜",
    [1, 3, 5],
    index=1,
    key=f"{CATEGORY_NAME}_n_items"
)
has_vs       = "vectorstore" in st.session_state
with_context = cols[2].checkbox(
    "ì—…ë¡œë“œ ë¬¸ì„œ ê¸°ë°˜(ê¶Œì¥)",
    has_vs,
    key=f"{CATEGORY_NAME}_with_context"
)

# í”„ë¡¬í”„íŠ¸
QUIZ_PROMPT_MC = """\
ë‹¹ì‹ ì€ ë°˜ë„ì²´ ê³µì • ê³¼ëª©ì˜ êµìˆ˜ì…ë‹ˆë‹¤.
ì£¼ì œ: {category}
ë‚œì´ë„: ì´ˆê¸‰
ì¶œì œ ë¬¸í•­ ìˆ˜: {n_items}

{context}

ìš”êµ¬ì‚¬í•­:
- 4ì§€ì„ ë‹¤ ê°ê´€ì‹ ë¬¸ì œë¥¼ {n_items}ê°œ ìƒì„±
- ê° ë¬¸í•­ì€ ë°˜ë“œì‹œ ì•„ë˜ 'ì •í™•í•œ í˜•ì‹'ì„ ì§€í‚¬ ê²ƒ (ì¶”ê°€ í…ìŠ¤íŠ¸ ê¸ˆì§€)
- ë³´ê¸°ëŠ” A) B) C) D) ë¡œ í‘œì‹œ, ì •ë‹µì€ í•˜ë‚˜ë§Œ
- ê° ë¬¸í•­ì— ê°„ë‹¨í•œ í•´ì„¤ 1~2ë¬¸ì¥ í¬í•¨

[ì •í™•í•œ í˜•ì‹ ì˜ˆì‹œ â€” ì´ í‹€ì„ ê·¸ëŒ€ë¡œ ì§€í‚¬ ê²ƒ]
1) ì§ˆë¬¸ í…ìŠ¤íŠ¸
A) ë³´ê¸° A
B) ë³´ê¸° B
C) ë³´ê¸° C
D) ë³´ê¸° D
ì •ë‹µ: A
í•´ì„¤: í•œë‘ ë¬¸ì¥ ì„¤ëª…
"""

QUIZ_PROMPT_TXT = """\
ë‹¹ì‹ ì€ ë°˜ë„ì²´ ê³µì • ê³¼ëª©ì˜ êµìˆ˜ì…ë‹ˆë‹¤.
ì£¼ì œ: {category}
ë‚œì´ë„: ê³ ê¸‰
ì¶œì œ ë¬¸í•­ ìˆ˜: {n_items}

{context}

ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì£¼ì œì— ë§ëŠ” ëœë¤ ì„œìˆ í˜• ë¬¸ì œë¥¼ {n_items}ê°œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
ë¬¸í•­ì€ 1), 2), 3)... ì²˜ëŸ¼ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ í•œ ì¤„ì”© ì‹œì‘í•˜ì„¸ìš”.
ë‹µì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""

EVAL_PROMPT_TMPL = """\
ë‹¹ì‹ ì€ {category} ë¶„ì•¼ì˜ ì±„ì  ë³´ì¡°ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸í•­ê³¼ ìˆ˜í—˜ì ë‹µì•ˆì„ í‰ê°€í•˜ì„¸ìš”.

[ë¬¸í•­]
{question}

[ìˆ˜í—˜ì ë‹µì•ˆ]
{answer}

(ì„ íƒ) ì°¸ê³  ì»¨í…ìŠ¤íŠ¸:
{context}

í‰ê°€ ê¸°ì¤€:
- ì‚¬ì‹¤ ì¼ì¹˜ ì—¬ë¶€, í•µì‹¬ ê°œë… í¬í•¨ ì—¬ë¶€, ë…¼ë¦¬ì„±.
- ê°„ê²°íˆ 'ì •ë‹µ' ë˜ëŠ” 'ì˜¤ë‹µ'ìœ¼ë¡œ íŒì •í•˜ê³ , 2~3ë¬¸ì¥ì˜ í”¼ë“œë°± ì œê³µ.

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ì§€í‚¤ì„¸ìš”(ì¤„ë°”ê¿ˆ í¬í•¨, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€):
íŒì •: ì •ë‹µ|ì˜¤ë‹µ
í”¼ë“œë°±: <ë‘ì„¸ ë¬¸ì¥ í”¼ë“œë°±>
"""

# ë¬¸ì œ ìƒì„± ë²„íŠ¼ ë™ì‘
if st.button("ëœë¤ ë¬¸ì œ ìƒì„±", use_container_width=True):
    ph = st.empty()
    with ph.container():
        if hasattr(st, "status"):
            with st.status("ë¬¸ì œ ìƒì„± ì¤‘...", expanded=True) as status:
                status.update(label="ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘...", state="running")
                backend, model = get_llm_backend()
                context = gather_context(k=6, enabled=with_context, retriever=st.session_state.vectorstore.as_retriever(search_kwargs={"k": 6}) if has_vs else None)

                status.update(label="í”„ë¡¬í”„íŠ¸ êµ¬ì„±...", state="running")
                if difficulty == "ì´ˆê¸‰":
                    prompt = QUIZ_PROMPT_MC.format(
                        category=CATEGORY_NAME,
                        n_items=n_items,
                        context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)")
                    )
                else:
                    prompt = QUIZ_PROMPT_TXT.format(
                        category=CATEGORY_NAME,
                        n_items=n_items,
                        context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)")
                    )

                status.update(label="ë¬¸í•­ ìƒì„± ìš”ì²­...", state="running")
                raw = generate_with_openai(prompt, model) if backend == "openai" else generate_with_gemini(prompt, model)

                prev_texts = [p if isinstance(p, str) else p.get("q","") for p in st.session_state[hist_key]]

                if difficulty == "ì´ˆê¸‰":
                    cand = parse_mc_questions(raw, n_items)
                    uniques = []
                    for item in cand:
                        if not any(is_similar(item["q"], pt) for pt in prev_texts):
                            uniques.append(item)
                    if len(uniques) < n_items:
                        need = n_items - len(uniques)
                        status.update(label=f"ë³´ê°• ìƒì„± ({need}ê°œ)...", state="running")
                        raw2 = generate_with_openai(prompt, model) if backend == "openai" else generate_with_gemini(prompt, model)
                        cand2 = parse_mc_questions(raw2, need)
                        for it in cand2:
                            if len(uniques) >= n_items: break
                            if not any(is_similar(it["q"], pt) for pt in (prev_texts + [u["q"] for u in uniques])):
                                uniques.append(it)

                    st.session_state[f"{CATEGORY_NAME}_quiz_items"] = uniques
                    st.session_state[f"{CATEGORY_NAME}_quiz_mode"]  = "ì´ˆê¸‰"
                    st.session_state[hist_key].extend([u["q"] for u in uniques])

                else:  # ê³ ê¸‰(ì„œìˆ í˜•)
                    cand = extract_questions(raw, n_items)
                    uniques = []
                    for q in cand:
                        if not any(is_similar(q, pt) for pt in prev_texts):
                            uniques.append(q)
                    if len(uniques) < n_items:
                        need = n_items - len(uniques)
                        status.update(label=f"ë³´ê°• ìƒì„± ({need}ê°œ)...", state="running")
                        raw2 = generate_with_openai(prompt, model) if backend == "openai" else generate_with_gemini(prompt, model)
                        cand2 = extract_questions(raw2, need)
                        for q in cand2:
                            if len(uniques) >= n_items: break
                            if not any(is_similar(q, pt) for pt in (prev_texts + uniques)):
                                uniques.append(q)

                    st.session_state[f"{CATEGORY_NAME}_quiz_items"] = uniques
                    st.session_state[f"{CATEGORY_NAME}_quiz_mode"]  = "ê³ ê¸‰"
                    st.session_state[hist_key].extend(uniques)

                status.update(label="ì™„ë£Œ âœ…", state="complete")
        else:
            bar = st.progress(0)
            backend, model = get_llm_backend(); bar.progress(10)
            context = gather_context(k=6, enabled=with_context, retriever=st.session_state.vectorstore.as_retriever(search_kwargs={"k": 6}) if has_vs else None); bar.progress(20)
            if difficulty == "ì´ˆê¸‰":
                prompt = QUIZ_PROMPT_MC.format(category=CATEGORY_NAME, n_items=n_items, context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"))
            else:
                prompt = QUIZ_PROMPT_TXT.format(category=CATEGORY_NAME, n_items=n_items, context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"))
            raw = generate_with_openai(prompt, model) if backend == "openai" else generate_with_gemini(prompt, model); bar.progress(60)
            prev_texts = [p if isinstance(p, str) else p.get("q","") for p in st.session_state[hist_key]]
            if difficulty == "ì´ˆê¸‰":
                cand = parse_mc_questions(raw, n_items)
                uniques = [it for it in cand if not any(is_similar(it["q"], pt) for pt in prev_texts)]
                st.session_state[f"{CATEGORY_NAME}_quiz_items"] = uniques
                st.session_state[f"{CATEGORY_NAME}_quiz_mode"]  = "ì´ˆê¸‰"
                st.session_state[hist_key].extend([u["q"] for u in uniques])
            else:
                cand = extract_questions(raw, n_items)
                uniques = [q for q in cand if not any(is_similar(q, pt) for pt in prev_texts)]
                st.session_state[f"{CATEGORY_NAME}_quiz_items"] = uniques
                st.session_state[f"{CATEGORY_NAME}_quiz_mode"]  = "ê³ ê¸‰"
                st.session_state[hist_key].extend(uniques)
            bar.progress(100)
    ph.empty()

# ë¬¸ì œ í‘œì‹œ ë‹µì•ˆ ì…ë ¥  ì±„ì 
items = st.session_state.get(f"{CATEGORY_NAME}_quiz_items", [])
mode  = st.session_state.get(f"{CATEGORY_NAME}_quiz_mode", "ê³ ê¸‰")

if items:
    st.markdown("### ìƒì„±ëœ ë¬¸ì œ")

    if mode == "ì´ˆê¸‰":
        # ê°ê´€ì‹ ë Œë”ë§
        for i, it in enumerate(items, start=1):
            st.markdown(f"**{i}) {it['q']}**")
            key = f"{CATEGORY_NAME}_mc_{i-1}"
            choice = st.radio("ë³´ê¸° ì„ íƒ", options=it["opts"], key=key, index=None)
            st.caption("ì •ë‹µ ì„ íƒ í›„ ì•„ë˜ 'ì±„ì í•˜ê¸°'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        if st.button("ì±„ì í•˜ê¸°", type="primary", use_container_width=True):
            st.markdown("### ì±„ì  ê²°ê³¼")
            for i, it in enumerate(items, start=1):
                key = f"{CATEGORY_NAME}_mc_{i-1}"
                sel = st.session_state.get(key)
                sel_letter = sel.split(")")[0] if sel else None
                correct = (sel_letter == it["answer"])
                verdict = "ì •ë‹µ" if correct else "ì˜¤ë‹µ"
                st.markdown(f"**ë¬¸í•­ #{i} ê²°ê³¼**")
                st.markdown(f"**íŒì •: {verdict}**")
                st.markdown(f"í”¼ë“œë°±: {it.get('expl','(í•´ì„¤ ì—†ìŒ)')}")
                st.markdown("---")
    else:
        # ì„œìˆ í˜•
        for i, qtext in enumerate(items, start=1):
            st.markdown(f"**{i}) {qtext}**")
            st.text_area(
                f"ë‹µì•ˆ ì…ë ¥ #{i}",
                key=f"{CATEGORY_NAME}_ans_{i-1}",
                height=100,
                placeholder="ì—¬ê¸°ì— ë³¸ì¸ ë‹µì•ˆì„ ì‘ì„±í•˜ì„¸ìš”."
            )
        # ì±„ì 
        if st.button("ì±„ì í•˜ê¸°", type="primary", use_container_width=True):
            backend, model = get_llm_backend()
            context = gather_context(k=6, enabled=with_context, retriever=st.session_state.vectorstore.as_retriever(search_kwargs={"k": 6}) if has_vs else None)
            results = []
            for i, qtext in enumerate(items):
                ans = st.session_state.get(f"{CATEGORY_NAME}_ans_{i}", "").strip()
                eval_prompt = EVAL_PROMPT_TMPL.format(
                    category=CATEGORY_NAME,
                    question=qtext,
                    answer=ans if ans else "(ë¬´ì‘ë‹µ)",
                    context=(f"[ì»¨í…ìŠ¤íŠ¸]\n{context}" if context else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)")
                )
                judged = generate_with_openai(eval_prompt, model) if backend == "openai" else generate_with_gemini(eval_prompt, model)
                results.append(judged)

            st.markdown("### ì±„ì  ê²°ê³¼")
            for i, judged in enumerate(results, start=1):
                st.markdown(f"**ë¬¸í•­ #{i} ê²°ê³¼**")
                v, fb = parse_eval(judged)
                st.markdown(f"**íŒì •: {v or 'íŒì • ë¶ˆëª…'}**")
                st.markdown(f"í”¼ë“œë°±: {fb or '(ì—†ìŒ)'}")
                st.markdown("---")
else:
    st.caption("ì•„ì§ ìƒì„±ëœ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤. â€˜ëœë¤ ë¬¸ì œ ìƒì„±â€™ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

